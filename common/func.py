import os
import json
import numpy as np
import pydicom
from glob import glob
from PIL import Image
from tqdm import tqdm

# Filter all files with incorrect orientation
def select_orientation(slice_file):
    
    orientation = slice_file[0x0020, 0x0037].value
    
    if orientation[0] == -1 or orientation[4] == -1:
        return False
    
    else:
        return True
    
# Filter all files by z-axis coordinate
def select_z(slice_file, z_coord, error_bound):
    z_axis_val = slice_file[0x0020, 0x0032].value[2]
    if z_axis_val > (z_coord + error_bound) or z_axis_val < (z_coord - error_bound):
        return False
    else:
        return True

# Filter out spiral CT images due to different acqusition process
def remove_spiral(slice_file):
    imtype = slice_file[0x0008, 0x0008]
    imtype = ''.join(imtype)
    if 'CT_SOM' in imtype:
        return False
    else:
        return True
    
# Stores the filtered list of files generated by "filter_files"
# into a "params.json" file
def store_params(param_dict, filepath=os.path.join(os.getcwd(), "params.json")):
    with open(filepath, "w") as f:
        return json.dump(param_dict, f)
    
    
# Load the "params.json" file as a dict
def load_params(filepath):
    with open(filepath, "r") as f:
        return json.load(f)
    
    
# Splits the patient into train and test sets
def train_test_split(pat_filepaths, prop_train=0.8, random_seed=None):

    # Extract all Patient IDs
    keys = list(pat_filepaths.keys())

    # Find total num of filepaths
    values_len = len([x for xs in pat_filepaths.values() for x in xs])
    # Find proportion of filepaths to go to train
    prop_ind = round(values_len*prop_train)
    print(prop_ind)

    # Shuffle Patient IDs
    rng = np.random.default_rng(seed=random_seed)
    rng.shuffle(keys)

    train_list = []
    test_list = []
    curr_ind = 0
    for key in keys:
        pat_fp_list = pat_filepaths[key]
        if (values_len-100) > curr_ind:
            train_list += pat_fp_list
        else:
            test_list += pat_fp_list
        curr_ind += len(pat_fp_list)

    rng.shuffle(train_list)
    rng.shuffle(test_list)

    return train_list, test_list


def get_item(filepaths, tag0, tag1):

    out_list = []
    
    for slice_filepath in filepaths:
        slice_file = pydicom.dcmread(slice_filepath)
        out_list.append(slice_file[tag0, tag1].value)

    return out_list

'''
Function loads all valid .dcm files in a directory, 
returns filepaths of CT images only.

Additionally, allows to pass functions that will constitute
additional filtering conditionals.

Functions must be of the form:  

  func(pydicom_file) -> bool
  
Must return False for the file to be excluded
'''
        
def preprocessor(directory = os.getcwd(), *args, prop_train=0.8, random_seed=None):

    out_dict = {}
    pat_ids = set()
    pat_filepaths = {}

    # Load all files present in the directory
    file_dirs = glob(f"{directory}/**/*.dcm", recursive=True)

    # filtered_dir_list = []
    pix_spac = []
    print("Initial File Selection:")
    for slice_filepath in tqdm(file_dirs):
        slice_file = pydicom.dcmread(slice_filepath) # Load image with pydicom
        
        if slice_file[0x0008,0x0060].value == "CT": # Load CT images only

            # Filters each slice through a set of bool functions
            check = True
            for arg in args:
                check = check and arg(slice_file) 
            if check == False:
                continue

            pat_id = slice_file[0x0010, 0x0020].value

            if pat_id in pat_ids:
                pat_filepaths[pat_id].append(slice_filepath)
            else:
                pat_ids.add(pat_id)
                pat_filepaths[pat_id] = [slice_filepath]
            
            pix_spac.append(slice_file[0x0028, 0x0030].value)
            # filtered_dir_list.append(slice_filepath)

    train_dir_list, test_dir_list = train_test_split(pat_filepaths=pat_filepaths,
                                                     prop_train=prop_train,
                                                     random_seed=random_seed)
    
    print(f"Total number of patients: {len(pat_ids)}")
    print(f"Total number of slices: {len(test_dir_list) + len(train_dir_list)}")
    print(f"Train number of slices: {len(train_dir_list)}")
    print(f"Test number of slices: {len(test_dir_list)}")
    
    out_dict["train_filepaths"] = train_dir_list
    out_dict["test_filepaths"] = test_dir_list
    out_dict["diam_lowq"] = np.quantile(get_item(train_dir_list, 0x0018, 0x1100), 0.4)
    out_dict["diam_highq"] = np.quantile(get_item(train_dir_list, 0x0018, 0x1100), 0.6)
    out_dict["pix_space"] = np.mean(pix_spac, axis = 0).tolist()

    return out_dict

# Function to normalize array to min_val -> max_val
def normalization(array, min_val=0, max_val=1):
    norm_array = (max_val-min_val)/(array.max() - array.min())*(array - array.min()) + min_val
    return norm_array

# Rescale image to "dims" pixels
def resize(array, dims):
    im_array = Image.fromarray(array)
    im_resized = im_array.resize(dims)
    img = np.array(im_resized)
    return img

# Set out of range pixels to zero
def zero_oor(img):
    # Find image centre
    centre = (img.shape[0]-1)/2
    
    # Set all out of range pixels to 0
    for i in range(img.shape[0]):
        for j in range(img.shape[1]):
            arg_dist = np.floor(np.sqrt((centre - i)**2 + (centre - j)**2))
            if arg_dist > centre:
                img[i, j] = 0    

    return img

def zoom(array, diameter, diameter_bounds):
    array_shape = array.shape
    diameter_bounds = np.sort(diameter_bounds)

    if diameter < diameter_bounds[0]:
        ratio_down = diameter_bounds[0]/diameter
        new_size = np.round(ratio_down*np.array(array_shape)).astype('int16')

        offset_x = int(new_size[0] - array_shape[0])
        if offset_x % 2 == 0:
            offset_x = round(offset_x/2)
            opp_offset_x = offset_x
        else:
            offset_x = round((offset_x - 1)/2)
            opp_offset_x = offset_x + 1

        offset_y = int(new_size[1] - array_shape[1])
        if offset_y % 2 == 0:
            offset_y = round(offset_y/2)
            opp_offset_y = offset_y
        else:
            offset_y = round((offset_y - 1)/2)
            opp_offset_y = offset_y + 1

        temp_arr = np.zeros(new_size)
        temp_arr[offset_x:temp_arr.shape[0]-opp_offset_x, offset_y:temp_arr.shape[1]-opp_offset_y] = array[:, :]


    elif diameter > diameter_bounds[1]:
        ratio_up = diameter_bounds[1]/diameter
        new_size = np.round(ratio_up*np.array(array_shape)).astype('int16')

        offset_x = int(array_shape[0] - new_size[0])
        if offset_x % 2 == 0:
            offset_x = round(offset_x/2)
            opp_offset_x = offset_x
        else:
            offset_x = round((offset_x - 1)/2)
            opp_offset_x = offset_x + 1

        offset_y = int(array_shape[1] - new_size[1])
        if offset_y % 2 == 0:
            offset_y = round(offset_y/2)
            opp_offset_y = offset_y
        else:
            offset_y = round((offset_y - 1)/2)
            opp_offset_y = offset_y + 1

        temp_arr = np.zeros(new_size)
        temp_arr[:, :] = array[offset_x:array_shape[0]-opp_offset_x, offset_y:array_shape[1]-opp_offset_y]


    else:
        temp_arr = array

    return temp_arr

# MCMC analysis algorithms

# All functions take raveled samples

# Find mean of a set of samples
def find_mean(samples):

    return np.mean(samples, axis = 0)

# Sort by probability-like of sample
def sort_by_mode(samples, log_pdf_like):

    prob_like = np.apply_along_axis(log_pdf_like, axis=1, arr=samples)
    arg = np.argsort(prob_like)
    sorted_samples = samples[arg, :]

    return sorted_samples

## Autocorrelation diagnosis

def c_coeff(f_samples, tau):

    N = f_samples.shape[0]
    N_tau = N - tau

    f_samples_norm = f_samples - f_samples.mean(axis=0)

    c_f = (1/N_tau)*np.multiply(f_samples_norm[:N_tau], f_samples_norm[N_tau:N])
    c_f = np.sum(c_f, axis=(0,1))

    return c_f

def tau_coeff(f_samples, C=5):

    M = 1
    tau_f_M = 1
    c_coeff_0 = c_coeff(f_samples, 0)
    while True:
        tau_f_M += 2*(c_coeff(f_samples, M)/c_coeff_0)
        if M >= C*tau_f_M:
            break
        else:
            M += 1
    return tau_f_M

# Accuracy metrics
def MSE(ground_truth, sample):

    SE = np.sum((ground_truth - sample)**2)
    Norm = ground_truth.ravel().shape[0]

    Mean_SE = (1/Norm)*SE
    return Mean_SE


def PSNR(ground_truth, sample):
    m = sample.max()

    SE = np.sum((ground_truth - sample)**2)
    Norm = ground_truth.ravel().shape[0]

    Mean_SE = (1/Norm)*SE
    return 10*np.log10(m**2/Mean_SE)


# SSIM 

def covariance(x, y):
    xbar, ybar = np.mean(x), np.mean(y)
    return np.sum((x - xbar)*(y - ybar))/(len(x) - 1)

def SSIM(ground_truth, sample):
    ground_truth_mean, sample_mean = np.mean(ground_truth), np.mean(sample)
